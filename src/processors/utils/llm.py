# COPYRIGHT © 2024 by Spring Health Corporation <office(at)springhealth.org>
# Toronto, Ontario, Canada
# SUMMARY: This file is part of the Get Well Clinic's original "AI-MOA" project's collection of software,
# documentation, and configuration files.
# These programs, documentation, and configuration files are made available to you as open source
# in the hopes that your clinic or organization may find it useful and improve your care to the public
# by reducing administrative burden for your staff and service providers.
# NO WARRANTY: This software and related documentation is provided "AS IS" and WITHOUT ANY WARRANTY of any kind;
# and WITHOUT EXPRESS OR IMPLIED WARRANTY OF SUITABILITY, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE.
# LICENSE: This software is licensed under the "GNU Affero General Public License Version 3".
# Please see LICENSE file for full details. Or contact the Free Software Foundation for more details.
# ***
# NOTICE: We hope that you will consider contributing to our common source code repository so that
# others may benefit from your shared work.
# However, if you distribute this code or serve this application to users in modified form,
# or as part of a derivative work, you are required to make your modified or derivative work
# source code available under the same herein described license.
# Please notify Spring Health Corp <office(at)springhealth.org> where your modified or derivative work
# source code can be acquired publicly in its latest most up-to-date version, within one month.
# ***

import datetime
import requests
from requests.exceptions import Timeout, RequestException

def query_prompt(self,prompt):
    """
    Sends a prompt to the AI model and retrieves the generated response.

    This method formats a prompt with the current date and specified configuration values, then sends a 
    request to the AI model using the provided API endpoint. The response content from the model is 
    extracted and returned.

    The method constructs a request body with the following data:
    - The current date is included in the prompt.
    - Configuration values such as the chat template, model, temperature, character, and top-p are 
      retrieved from the application settings.
    
    The request is made via a POST request to a specified URL, and the response content from the model 
    is returned as part of a tuple.

    Args:
        prompt (str): The prompt text to be sent to the AI model.

    Returns:
        tuple: 
            - `True` if the request is successful.
            - The content of the response generated by the AI model.
        
        Example:
            >>> result = query_prompt("What is the weather today?")
            >>> print(result)
            (True, "The weather is sunny with a temperature of 25°C.")
        
    Raises:
        requests.exceptions.RequestException: If there is an issue with the HTTP request.
    """
    data = {
        "messages": [
            {
                "role": "user",
                "content": f"Today's Date is : {datetime.datetime.now().date()}\n. {prompt}"
            }
        ],
        "chat_template": self.config.get('llm.chat_template'),
        "model": self.config.get('llm.model'),
        "temperature": self.config.get('llm.temperature'),
        "character": self.config.get('llm.character'),
        "top_p": self.config.get('llm.top_p')
    }
    log_llm_response = self.config.get('llm.log_responses', False)

    try:
        response = requests.post(self.url, headers=self.headers, json=data, verify=self.config.get('ai.verify-HTTPS'), timeout=self.config.get('general_setting.timeout', 300))
    except Timeout:
        self.config.update_lock_status(False)
        self.logger.info(f"Lock released.")
        self.logger.info(f"An error occurred waiting for LLM response, exceeded time out. Stopping task processing Document No. {self.file_name}")
        raise SystemExit("Stopping task due to LLM timed out.")
    except RequestException as e:
        self.config.update_lock_status(False)
        self.logger.info(f"Lock released.")
        self.logger.info(f"An error occurred waiting for LLM response, LLM Request Exception. Stopping task processing Document No. {self.file_name}")
        raise SystemExit("Stopping task due to LLM Request Exception.")
    else:
        if response.status_code != 200:
            return False
        content_value = response.json()['choices'][0]['message']['content']
        if log_llm_response:
            print('#### LLM Response ####')
            print(content_value)
            print('#### End of Response ####')
        return True, content_value